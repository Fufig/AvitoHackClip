# Avito: Сравнение возможных решений для классификации по цвету

---

### Сравнение архитектур: CLIP vs CNN + сегментация + предобработка

#### CLIP
- Может работать с текстовыми запросами (поддерживает описание товара и текстовый поиск)
- Не требует ручной сегментации и сложной предобработки
- Устойчив к шуму и грязным данным
- Хорошо обобщается на объекты, которых не было в датасете
- Масштабируем под разные задачи с минимумом усилий

#### CNN + сегментация + предобработка
- Требует ресурсов и времени на предобработку (сегментация, нормализация)
- Сегментация может не справляться с грязными данными
- Нужны хорошо размеченные обучающие данные
- Плохо обобщается на редкие классы
- Обучается под конкретную задачу, плохо переносится

---

### Почему выбран CLIP
- Подходит для задач Авито: товары описаны нестрого, пользовательский запрос — текстовый
- Масштабируем: может использоваться как фильтр для поиска, работает быстрее и точнее, чем текст-текст поиск
- Благодаря высокой скорости позволяет не размечать весь датасет — можно применять поиск по тексту в реальном времени
- Устойчив к новым, ранее не встречавшимся данным

---

### Проблема базового CLIP

На основе статей:
1. https://arxiv.org/html/2212.08650v2
2. https://library.imaging.org/cic/articles/32/1/20

Выяснилось, что базовая CLIP недостаточно чувствительна к цвету: реагирует в основном на форму объекта, а не его цвет. Это критично для нашей задачи.

---

### Как решали проблему цветовой нечувствительности

- Мы изучили архитектуры, аналогичные CLIP, но обученные на датасетах с акцентом на цвета
- Выбрана модель из статьи https://arxiv.org/pdf/2405.20204, обученная на ShareGPT4V и MTEB (внимание к цветам)
- Наш датасет имеет особенности (оттенки, названия цветов), поэтому мы дообучили модель:
  - Использовали 20% от датасета
  - Обучение в 5 эпохах (ограничения по ресурсам и времени)

---

### Ход решения (подробно)

1. **Формулировка задачи**:
   - Цель: автоматическая классификация изображения товара по одному из 18 предопределенных цветов
   - Входные данные: изображение и текстовое описание с Авито

2. **Оценка архитектурных вариантов**:
   - Сравнили CLIP и связку CNN + сегментация
   - Отказались от CNN из-за сложности предобработки и плохой обобщаемости

3. **Выбор CLIP**:
   - Подходит под мультимодальность задачи (изображение + текст)
   - Хорошо масштабируется и устойчив к шуму

4. **Проблема базовой CLIP — слабая чувствительность к цвету**:
   - Подтверждено в исследованиях
   - Требуется адаптация модели к задаче цветовой классификации

5. **Поиск подходящей архитектуры и данных**:
   - Найдена модель из [https://arxiv.org/pdf/2405.20204], обученная на цветочувствительных датасетах
   - Подходит для дальнейшего дообучения

6. **Дообучение (fine-tuning)**:
   - Использовали часть собственного датасета с разметкой по цветам
   - Дообучение: 5 эпох на 20% данных из-за ограниченных ресурсов
   - Получили точность ~80% при 18 классах (оценивали на предоставленнных, грязных данных, где присвоенный класс не всегла верен)

7. **Анализ результатов и перспективы**:
   - Потенциал улучшения при более полном и чистом датасете
   - Модель устойчива к редким случаям ("фиолетовая кошка", "золотой унитаз")
   - Хорошо подходит для фильтрации товаров по текстовому запросу

---

### Решение

- `clip-train-avito.ipynb`: ознакомительное обучение эмбеддеров с нуля, для понимания работы архитектуры CLIP. Не используется в основном пайплайне.
- `clip-img-embedder-tuner.ipynb`: дообучение визуального энкодера CLIP по цветам и формам объектов, чтобы учесть специфику формы товара
- `CLIP_last_layer_tuner.ipynb`: финальный тюнинг последних слоёв модели для классификации по 18 цветам

---

### Результаты и выводы

- Дообученная модель (визуальный трансформер + текстовый энкодер) достигла ~50% accuracy на 18 классах
- Потенциал значительного улучшения при более чистом и полном датасете
- Модель устойчива к необычным комбинациям (например, "фиолетовая кошка" или "золотой унитаз") при достаточном обучении
- Хорошо масштабируется под задачи поиска и фильтрации по тексту с минимальными затратами на поддержку
