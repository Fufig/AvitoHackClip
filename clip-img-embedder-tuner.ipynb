{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10992345,"sourceType":"datasetVersion","datasetId":6842063}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision.datasets import CIFAR10\nfrom PIL import Image, ImageDraw, ImageOps\nimport random\n\nfrom transformers import CLIPProcessor, CLIPModel\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:48:33.708689Z","iopub.execute_input":"2025-03-13T17:48:33.708975Z","iopub.status.idle":"2025-03-13T17:48:53.602022Z","shell.execute_reply.started":"2025-03-13T17:48:33.708952Z","shell.execute_reply":"2025-03-13T17:48:53.601305Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:48:53.603004Z","iopub.execute_input":"2025-03-13T17:48:53.603553Z","iopub.status.idle":"2025-03-13T17:50:34.724088Z","shell.execute_reply.started":"2025-03-13T17:48:53.603527Z","shell.execute_reply":"2025-03-13T17:50:34.723375Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90bfc21d3b8a4c97bff719ad23756004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c908b92191a4471298e39c5c5645cc72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1acfb58ff945a2a696ee0123431e78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9675c5501c4e4f69b9da31a0a3a97913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435df291c2244a338ac90240dd46b258"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fcffd6b888645ad86ecdaa4458153c3"}},"metadata":{}},{"name":"stderr","text":"Error while downloading from https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/merges.txt: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\nTrying to resume download...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2da1027984141ddb1c82c0e39165962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bbe514a4aa04df39e7e2c1620dfa295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7479d223fb64fb68a6a4324311a4736"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ncategories_dict = {\"сумки\": \"bags\", \"столы\": \"tables\", \"одежда для девочек\": \"clothes for girls\", \"стулья\": \"chairs\"}\ncolors_dict = {\"zelenyi\": \"green\", \"chernyi\": \"black\", \"belyi\": \"white\", \"bordovyi\": \"burgundy\", \"krasnyi\": \"red\", \"bezhevyi\": \"beige\", \"raznocvetnyi\": \"multicolored\", \"rozovyi\": \"pink\", \"serebristyi\": \"silver\", \"korichnevyi\": \"brown\", \"fioletovyi\": \"purple\", \"seryi\": \"gray\", \"goluboi\": \"light blue\", \"oranzhevyi\": \"orange\", \"sinii\": \"blue\", \"biryuzovyi\": \"turquoise\", \"zolotoi\": \"gold\", \"zheltyi\": \"yellow\"}\n\ndf = pd.read_csv('/kaggle/input/avitohack/dataset_colors/train_data.csv')\ndf['text'] = [f'{colors_dict[i[2]]} {categories_dict[i[1]]}' for i in df.values]\ndf['path'] = [f'/kaggle/input/avitohack/dataset_colors/train_data/{i[0]}.jpg' for i in df.values]\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:50:34.725651Z","iopub.execute_input":"2025-03-13T17:50:34.725953Z","iopub.status.idle":"2025-03-13T17:50:34.844058Z","shell.execute_reply.started":"2025-03-13T17:50:34.725929Z","shell.execute_reply":"2025-03-13T17:50:34.843246Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                id            category        target  \\\n0      11624739720               сумки       zelenyi   \n1      11967354953               сумки       chernyi   \n2      19026830493               сумки         belyi   \n3      23088722867               сумки       chernyi   \n4      23546690270               столы         belyi   \n...            ...                 ...           ...   \n33298  37599243178  одежда для девочек  raznocvetnyi   \n33299  37599258032               сумки      bezhevyi   \n33300  37599262080  одежда для девочек      bezhevyi   \n33301  37599266008  одежда для девочек      bezhevyi   \n33302  37599275027               сумки       chernyi   \n\n                                 text  \\\n0                          green bags   \n1                          black bags   \n2                          white bags   \n3                          black bags   \n4                        white tables   \n...                               ...   \n33298  multicolored clothes for girls   \n33299                      beige bags   \n33300         beige clothes for girls   \n33301         beige clothes for girls   \n33302                      black bags   \n\n                                                    path  \n0      /kaggle/input/avitohack/dataset_colors/train_d...  \n1      /kaggle/input/avitohack/dataset_colors/train_d...  \n2      /kaggle/input/avitohack/dataset_colors/train_d...  \n3      /kaggle/input/avitohack/dataset_colors/train_d...  \n4      /kaggle/input/avitohack/dataset_colors/train_d...  \n...                                                  ...  \n33298  /kaggle/input/avitohack/dataset_colors/train_d...  \n33299  /kaggle/input/avitohack/dataset_colors/train_d...  \n33300  /kaggle/input/avitohack/dataset_colors/train_d...  \n33301  /kaggle/input/avitohack/dataset_colors/train_d...  \n33302  /kaggle/input/avitohack/dataset_colors/train_d...  \n\n[33303 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>category</th>\n      <th>target</th>\n      <th>text</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11624739720</td>\n      <td>сумки</td>\n      <td>zelenyi</td>\n      <td>green bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11967354953</td>\n      <td>сумки</td>\n      <td>chernyi</td>\n      <td>black bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19026830493</td>\n      <td>сумки</td>\n      <td>belyi</td>\n      <td>white bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23088722867</td>\n      <td>сумки</td>\n      <td>chernyi</td>\n      <td>black bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23546690270</td>\n      <td>столы</td>\n      <td>belyi</td>\n      <td>white tables</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33298</th>\n      <td>37599243178</td>\n      <td>одежда для девочек</td>\n      <td>raznocvetnyi</td>\n      <td>multicolored clothes for girls</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>33299</th>\n      <td>37599258032</td>\n      <td>сумки</td>\n      <td>bezhevyi</td>\n      <td>beige bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>33300</th>\n      <td>37599262080</td>\n      <td>одежда для девочек</td>\n      <td>bezhevyi</td>\n      <td>beige clothes for girls</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>33301</th>\n      <td>37599266008</td>\n      <td>одежда для девочек</td>\n      <td>bezhevyi</td>\n      <td>beige clothes for girls</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n    <tr>\n      <th>33302</th>\n      <td>37599275027</td>\n      <td>сумки</td>\n      <td>chernyi</td>\n      <td>black bags</td>\n      <td>/kaggle/input/avitohack/dataset_colors/train_d...</td>\n    </tr>\n  </tbody>\n</table>\n<p>33303 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Function to resize image to the required size\ndef resize_image(image, size):\n    return image.resize(size, Image.LANCZOS)\n\nclass CIFAR10ClipDataset(Dataset):\n    def __init__(self, cifar_dataset, num_samples=None, transform=None):\n        self.cifar_dataset = cifar_dataset\n        self.transform = transform\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples if self.num_samples else len(self.cifar_dataset)\n\n    def __getitem__(self, idx):\n        if self.num_samples:\n            idx1 = random.randint(0, len(self.cifar_dataset) - 1)\n            idx2 = random.randint(0, len(self.cifar_dataset) - 1)\n        else:\n            idx1 = idx\n            idx2 = random.randint(0, len(self.cifar_dataset) - 1)\n\n        \n        img1, label1 = self.cifar_dataset.iloc[idx1][['path', 'text']]\n        img2, label2 = self.cifar_dataset.iloc[idx2][['path', 'text']]\n\n        img1, img2 = Image.open(img1).convert('RGB'), Image.open(img2).convert('RGB')\n\n        img1, img2 = self.resize_and_pad(img1), self.resize_and_pad(img2)\n\n        img1, img2 = self.transform(img1), self.transform(img2)\n\n        # Determine if the images are from the same category\n        if label1 == label2:\n            return img1, img2, torch.tensor(1)  # Pair from the same category\n        else:\n            return img1, img2, torch.tensor(0)  # Pair from different categories\n\n    def resize_and_pad(self, image, target_size=224):\n        original_width, original_height = image.size\n        aspect_ratio = original_width / original_height\n\n        if aspect_ratio > 1:\n            new_width = target_size\n            new_height = int(target_size / aspect_ratio)\n        else:\n            new_height = target_size\n            new_width = int(target_size * aspect_ratio)\n\n        image = image.resize((new_width, new_height))\n\n        pad_width = (target_size - new_width) // 2\n        pad_height = (target_size - new_height) // 2\n\n        padding = (pad_width, pad_height, target_size - new_width - pad_width, target_size - new_height - pad_height)\n        image = ImageOps.expand(image, padding, fill=(0, 0, 0))\n\n        return image\n\n    \n\ntransform = transforms.Compose([\n    transforms.ToTensor(),           # Convert images to PyTorch tensors\n])\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:50:34.845134Z","iopub.execute_input":"2025-03-13T17:50:34.845451Z","iopub.status.idle":"2025-03-13T17:50:34.854428Z","shell.execute_reply.started":"2025-03-13T17:50:34.845429Z","shell.execute_reply":"2025-03-13T17:50:34.853576Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n\ncifar_clip_dataset = CIFAR10ClipDataset(df, num_samples=128*50, transform=transform)\nbatch_size = 128\ndataloader = DataLoader(cifar_clip_dataset, batch_size=batch_size, shuffle=True)\n\nclass CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n\n    def forward(self, emb1, emb2, labels):\n        similarities = torch.nn.functional.cosine_similarity(emb1, emb2)\n        \n        loss = torch.mean((1 - labels) * similarities + labels * (1 - similarities))\n        return loss\n\ncriterion = CustomLoss()\n\noptimizer = optim.Adam(model.parameters(), lr=1e-6)\n\nmodel.to(device)\n\nepochs = 1\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    with tqdm(enumerate(dataloader, 0), total=len(dataloader), desc=f\"Epoch [{epoch+1}/{epochs}]\") as tepoch:\n        for i, data in tepoch:\n            img1, img2, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n            \n            emb1 = model.get_image_features(img1)\n            emb2 = model.get_image_features(img2)\n            \n            loss = criterion(emb1, emb2, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            tepoch.set_postfix(loss=loss.item())\n            running_loss += loss.item()\n            \n    avg_loss = running_loss / len(dataloader)\n    print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"clip_tuned_by_similarity.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:08:19.905555Z","iopub.execute_input":"2025-03-13T20:08:19.905885Z","iopub.status.idle":"2025-03-13T20:08:20.724369Z","shell.execute_reply.started":"2025-03-13T20:08:19.905855Z","shell.execute_reply":"2025-03-13T20:08:20.723688Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class TestDataaste(Dataset):\n    def __init__(self, cifar_dataset, num_samples=None, transform=None):\n        self.cifar_dataset = cifar_dataset\n        self.transform = transform\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples if self.num_samples else len(self.cifar_dataset)\n\n    def __getitem__(self, idx1):\n\n        \n        img1, label1 = self.cifar_dataset.iloc[idx1][['path', 'text']]\n\n        img1 = Image.open(img1).convert('RGB')\n\n        img1= self.resize_and_pad(img1)\n\n        img1 = self.transform(img1)\n   \n        return img1, label1\n\n    def resize_and_pad(self, image, target_size=224):\n        original_width, original_height = image.size\n        aspect_ratio = original_width / original_height\n\n        if aspect_ratio > 1:\n            new_width = target_size\n            new_height = int(target_size / aspect_ratio)\n        else:\n            new_height = target_size\n            new_width = int(target_size * aspect_ratio)\n\n        image = image.resize((new_width, new_height))\n\n        pad_width = (target_size - new_width) // 2\n        pad_height = (target_size - new_height) // 2\n\n        padding = (pad_width, pad_height, target_size - new_width - pad_width, target_size - new_height - pad_height)\n        image = ImageOps.expand(image, padding, fill=(0, 0, 0))\n\n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:19:59.044939Z","iopub.execute_input":"2025-03-13T20:19:59.045294Z","iopub.status.idle":"2025-03-13T20:19:59.052234Z","shell.execute_reply.started":"2025-03-13T20:19:59.045261Z","shell.execute_reply":"2025-03-13T20:19:59.051306Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score\n\ndef evaluate_accuracy(model, test_loader, device=\"cuda\"):\n    model.to(device)\n    model.eval()  # Переводим модель в режим оценки\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images) \n            _, preds = torch.max(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T21:11:44.859388Z","iopub.execute_input":"2025-03-13T21:11:44.859721Z","iopub.status.idle":"2025-03-13T21:11:44.865034Z","shell.execute_reply.started":"2025-03-13T21:11:44.859691Z","shell.execute_reply":"2025-03-13T21:11:44.864096Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"cifar10_classes = df.text.unique()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n\ncifar_clip_dataset = TestDataaste(df.sample(1000),transform=transform)\ntest_loader = DataLoader(cifar_clip_dataset, batch_size=64, shuffle=False)\n\n\naccuracy = evaluate_accuracy(model, test_loader, device=\"cuda\")\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}